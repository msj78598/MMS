# -*- coding: utf-8 -*-
# mms_anomaly_crosscheck.py
import re
import math
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime

st.set_page_config(page_title="MMS | Cross-Check Anomalies vs Open Tasks", layout="wide")

# -------------------------------
# Helpers
# -------------------------------
def norm_col(c: str) -> str:
    return re.sub(r"\s+", " ", str(c).strip()).lower()

# ØµÙŠØ§Ù†Ø©: Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
CLOSED_CANDS = ["Task Closed Time", "Task Completed Time", "ÙˆÙ‚Øª Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©"]
REG_CANDS    = ["Task Registration Date Time", "Request Registration Date Time", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨"]
STATUS_CANDS = ["Task Status", "Request Status", "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©", "Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨"]
METER_CANDS  = ["Meter No", "Meter Number", "Ø±Ù‚Ù… Ø§Ù„Ø¹Ø¯Ø§Ø¯"]
VIP_CANDS    = ["Subscription VIP", "VIP", "Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ", "Account Type", "ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´ØªØ±Ùƒ"]

BUCKET_HINTS = {
    "Ø§Ø³ØªØ¨Ø¯Ø§Ù„": ["Ø§Ø³ØªØ¨Ø¯Ø§Ù„"],
    "ØªØ­Ø³ÙŠÙ†": ["ØªØ­Ø³ÙŠÙ†", "Ø§Ø³ØªØ®Ø±Ø§Ø¬", "ØªØ­Ø³ÙŠÙ† ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬"],
    "ØµÙŠØ§Ù†Ø©": ["ØµÙŠØ§Ù†Ø©"],
    "ÙƒØ´Ù": ["ÙƒØ´Ù", "Ù…Ø¹Ø§ÙŠÙ†Ø©", "ÙƒØ´Ù ÙˆÙ…Ø¹Ø§ÙŠÙ†Ø©"]
}

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
    nm = {norm_col(c): c for c in df.columns}
    for c in candidates:
        if norm_col(c) in nm:
            return nm[norm_col(c)]
    # ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ Ø§Ø­ØªÙŠØ§Ø·ÙŠ
    for c in df.columns:
        for cand in candidates:
            if norm_col(cand) in norm_col(c):
                return c
    return None

def infer_bucket_from_name(name: str) -> str:
    n = (name or "").lower()
    for b, kws in BUCKET_HINTS.items():
        for kw in kws:
            if kw in n:
                return b
    return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

# Ø´Ø°ÙˆØ°: Ù…Ù† Ù…Ù„ÙÙƒ (Ø£Ù‡Ù… Ø¹Ù…ÙˆØ¯ Meter Number + Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø´Ø°ÙˆØ°)
ANOM_METER_CANDS = ["Meter Number", "Meter No", "Meter", "Ø±Ù‚Ù… Ø§Ù„Ø¹Ø¯Ø§Ø¯"]
ANOM_SEV_CANDS   = ["final_label", "Severity", "Ø®Ø·ÙˆØ±Ø©", "Ø£ÙˆÙ„ÙˆÙŠØ©"]
ANOM_LOSS_CANDS  = ["confirmed_loss", "suspected_loss", "bypass_score", "Loss kWh", "Estimated Loss", "ÙØ§Ù‚Ø¯"]

# -------------------------------
# Readers
# -------------------------------
def read_maintenance(file):
    xl = pd.ExcelFile(file)
    sheet = xl.sheet_names[0]
    df = xl.parse(sheet)

    closed_col = pick_col(df, CLOSED_CANDS)
    reg_col    = pick_col(df, REG_CANDS)
    status_col = pick_col(df, STATUS_CANDS)
    meter_col  = pick_col(df, METER_CANDS)
    vip_col    = pick_col(df, VIP_CANDS)

    if closed_col is not None:
        df[closed_col] = pd.to_datetime(df[closed_col], errors="coerce")
        df["Closed Date"] = df[closed_col].dt.date
    else:
        df["Closed Date"] = pd.NaT

    if reg_col is not None:
        df[reg_col] = pd.to_datetime(df[reg_col], errors="coerce")

    if "Bucket" not in df.columns:
        df["Bucket"] = infer_bucket_from_name(getattr(file, "name", ""))

    # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…ÙØªÙˆØ­/Ø§Ù„Ù…ØºÙ„Ù‚
    is_closed = df[closed_col].notna() if closed_col else df["Closed Date"].notna()
    df["__is_open__"] = ~is_closed

    # Ø¹Ù…Ø± Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø¨Ø§Ù„Ø£ÙŠØ§Ù…
    if reg_col is not None:
        df["Age Days"] = (pd.to_datetime(datetime.now()) - df[reg_col]).dt.days
    else:
        df["Age Days"] = np.nan

    meta = dict(
        sheet=sheet, closed_col=closed_col, reg_col=reg_col, status_col=status_col,
        meter_col=meter_col, vip_col=vip_col, columns=list(df.columns)
    )
    return df, meta

def read_anomalies(file):
    xl = pd.ExcelFile(file)
    sheet = xl.sheet_names[0]
    df = xl.parse(sheet)

    a_meter = pick_col(df, ANOM_METER_CANDS)
    a_sev   = pick_col(df, ANOM_SEV_CANDS)
    # Ù†Ø¬Ù…Ø¹ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ÙØ§Ù‚Ø¯ Ø¥Ù† ÙˆÙØ¬Ø¯Øª
    loss_cols = [c for c in df.columns if norm_col(c) in [norm_col(x) for x in ANOM_LOSS_CANDS]]

    meta = dict(sheet=sheet, a_meter=a_meter, a_sev=a_sev, loss_cols=loss_cols, columns=list(df.columns))
    return df, meta

# -------------------------------
# UI
# -------------------------------
st.title("ğŸ”§ MMS â€” Cross-Check Anomalies vs Open Maintenance")
st.caption("ØªØ¬Ù†Ù‘Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¨Ù„Ø§ØºØ§Øª: Ù†Ø·Ø§Ø¨Ù‚ Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø§Ø°Ø© Ù…Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ù„Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙ†Ø¹Ø·ÙŠÙƒ Ù‚Ø§Ø¦Ù…ØªÙŠÙ† (ØªØ³Ø±ÙŠØ¹ / ÙØªØ­ Ø¬Ø¯ÙŠØ¯).")

with st.sidebar:
    st.header("ğŸ“ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø©")
    maint_files = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© (ÙŠÙ…ÙƒÙ† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª)", type=["xlsx","xls"], accept_multiple_files=True)
    st.header("ğŸ“ Ù…Ù„Ù/Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø§Ø°Ø©")
    anom_files  = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù/Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø°ÙˆØ°", type=["xlsx","xls"], accept_multiple_files=True)
    st.markdown("---")
    sla_days = st.number_input("Ø­Ø¯ Ø§Ù„ØªØ£Ø®ÙŠØ± (SLA) Ø¨Ø§Ù„Ø£ÙŠØ§Ù…", min_value=1, max_value=60, value=3)

if not maint_files:
    st.info("âœ¨ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø© Ø£ÙˆÙ„Ø§Ù‹.")
    st.stop()
if not anom_files:
    st.info("âœ¨ Ø§Ø±ÙØ¹ Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø§Ø°Ø© (detailed_asdct...).")
    st.stop()

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ¶Ù…Ù‘Ù‡Ø§
m_dfs, m_metas = [], []
for f in maint_files:
    df, meta = read_maintenance(f)
    m_dfs.append(df)
    m_metas.append(meta)
maintenance = pd.concat(m_dfs, ignore_index=True, sort=False)

# Ø£Ø¹Ù…Ø¯Ø© Ù…Ø¹Ø±Ù‘ÙØ©
closed_col = next((m["closed_col"] for m in m_metas if m["closed_col"]), None)
reg_col    = next((m["reg_col"]    for m in m_metas if m["reg_col"]), None)
meter_col  = next((m["meter_col"]  for m in m_metas if m["meter_col"]), None)
vip_col    = next((m["vip_col"]    for m in m_metas if m["vip_col"]), None)

# ÙÙ„ØªØ±Ø© Ø§Ù„Ù…ÙØªÙˆØ­
open_tasks = maintenance[maintenance["__is_open__"]].copy()
open_tasks["Is Late"] = open_tasks["Age Days"] > sla_days

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø´Ø°ÙˆØ° ÙˆØ¶Ù…Ù‘Ù‡Ø§
a_dfs, a_metas = [], []
for f in anom_files:
    df, meta = read_anomalies(f)
    df["Anomaly Source"] = getattr(f, "name", "anomaly.xlsx")
    a_dfs.append(df)
    a_metas.append(meta)
anomalies = pd.concat(a_dfs, ignore_index=True, sort=False)

a_meter = next((m["a_meter"] for m in a_metas if m["a_meter"]), None)
a_sev   = next((m["a_sev"] for m in a_metas if m["a_sev"]), None)
# Ø§Ø¬Ù…Ø¹ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ§Ù‚Ø¯ Ø§Ù„Ù…ØªØ§Ø­Ø©
loss_cols = []
for m in a_metas:
    loss_cols.extend([c for c in m["loss_cols"]])
loss_cols = list(dict.fromkeys(loss_cols))

# -------------------------------
# Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
# -------------------------------
st.markdown("## 1) Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©")
c1,c2,c3,c4 = st.columns(4)
c1.metric("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©", f"{len(open_tasks):,}")
c2.metric("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©", f"{open_tasks['Is Late'].sum():,}")
c3.metric("Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø´Ø°ÙˆØ°", f"{len(anomalies):,}")
c4.metric("Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©", a_meter or "Meter Number (Ù…ÙÙ‚ÙˆØ¯)")

# -------------------------------
# Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© (Meter Number â†” Meter Number)
# -------------------------------
st.markdown("## 2) Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
if not meter_col or not a_meter:
    st.error("Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù… Ø§Ù„Ø¹Ø¯Ø§Ø¯ ÙÙŠ Ø£Ø­Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª. ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Meter Number ÙÙŠ Ù…Ù„Ù Ø§Ù„Ø´Ø°ÙˆØ° Ùˆ Meter No/Meter Number ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙŠØ§Ù†Ø©.")
    st.stop()

left = anomalies.copy()
right = open_tasks.copy()

left[a_meter]  = left[a_meter].astype(str).str.strip()
right[meter_col] = right[meter_col].astype(str).str.strip()

matches = left.merge(
    right,
    how="inner",
    left_on=a_meter,
    right_on=meter_col,
    suffixes=("_ANOM", "_TASK")
)

# -------------------------------
# Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
# -------------------------------
total_anom = len(anomalies)
matched_meters = matches[[a_meter]].drop_duplicates() if len(matches) else pd.DataFrame(columns=[a_meter])
num_matched = len(matched_meters)

# Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù„Ù‡Ø§ Ù…Ù‡Ø§Ù… Ù…ÙØªÙˆØ­Ø© (Ø§ÙØªØ­ Ø¨Ù„Ø§Øº Ø¬Ø¯ÙŠØ¯)
unmatched = anomalies.copy()
if num_matched:
    unmatched = anomalies.merge(matched_meters, how="left", on=a_meter, indicator=True)
    unmatched = unmatched[unmatched["_merge"]=="left_only"].drop(columns=["_merge"])

st.markdown("### Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©")
d1,d2,d3 = st.columns(3)
d1.metric("ÙˆØ¬Ø¯Ù†Ø§ Ù„Ù‡Ø§ Ù…Ù‡Ø§Ù… Ù…ÙØªÙˆØ­Ø© (ØªØ³Ø±ÙŠØ¹)", f"{num_matched:,}")
d2.metric("ØªØ­ØªØ§Ø¬ ÙØªØ­ Ù…Ù‡Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©", f"{len(unmatched):,}")
late_rate = (open_tasks["Is Late"].mean()*100) if len(open_tasks) else 0
d3.metric("Ù†Ø³Ø¨Ø© ØªØ£Ø®ÙŠØ± Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©", f"{late_rate:,.1f}%")

# Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ØªØ³Ø±ÙŠØ¹ Ø­Ø³Ø¨ VIP Ø¥Ù† Ù…ØªØ§Ø­
if vip_col and vip_col in matches.columns:
    st.markdown("#### Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„ØªØ³Ø±ÙŠØ¹ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ (VIP)")
    vip_escalate = (matches.groupby(vip_col).size().reset_index(name="OpenTasksForAnomalies")
                    .sort_values("OpenTasksForAnomalies", ascending=False))
    st.dataframe(vip_escalate, use_container_width=True)

# Ø¬Ø¯Ø§ÙˆÙ„ ØªÙØµÙŠÙ„ÙŠØ©
st.markdown("---")
st.markdown("### âœ… Ø­Ø§Ù„Ø§Øª Ø´Ø§Ø°Ø© Ù„Ù‡Ø§ Ù…Ù‡Ø§Ù… ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø© (Ø³Ø±Ù‘Ø¹ Ø§Ù„ØªÙ†ÙÙŠØ°)")
if len(matches):
    cols_show = [a_meter, a_sev, "Anomaly Source"] + loss_cols
    cols_show = [c for c in cols_show if c and c in matches.columns]
    # Ø£Ø¹Ù…Ø¯Ø© Ù…Ø®ØªØ§Ø±Ø© Ù…Ù† Ø§Ù„ØµÙŠØ§Ù†Ø©
    for c in ["Bucket", "Task Code", "Request id", "Task Status", reg_col, closed_col, meter_col, vip_col, "Age Days", "Is Late"]:
        if c and c in matches.columns:
            cols_show.append(c)
    cols_show = list(dict.fromkeys(cols_show))
    st.dataframe(matches[cols_show].sort_values(by=["Is Late","Age Days"] if "Age Days" in cols_show else cols_show[0], ascending=[False, False] if "Age Days" in cols_show else True), use_container_width=True)
else:
    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø§Ù„Ø§Øª Ø´Ø§Ø°Ø© Ù„Ù‡Ø§ Ù…Ù‡Ø§Ù… Ù…ÙØªÙˆØ­Ø© Ù„Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø§Ø¯ ÙˆÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")

st.markdown("### ğŸ†• Ø­Ø§Ù„Ø§Øª Ø´Ø§Ø°Ø© Ø¨Ù„Ø§ Ù…Ù‡Ø§Ù… Ù…Ø·Ø§Ø¨Ù‚Ø© (Ø§ÙØªØ­ Ø¨Ù„Ø§Øº ÙØ­Øµ)")
st.dataframe(unmatched, use_container_width=True)

# ØªÙ†Ø²ÙŠÙ„
st.markdown("---")
c_dl1, c_dl2 = st.columns(2)
with c_dl1:
    if len(matches):
        st.download_button(
            "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ³Ø±ÙŠØ¹ (CSV)",
            data=matches.to_csv(index=False).encode("utf-8-sig"),
            file_name="anomalies_with_open_tasks_to_escalate.csv",
            mime="text/csv"
        )
with c_dl2:
    st.download_button(
        "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ÙØªØ­ Ù…Ù‡Ù…Ø© (CSV)",
        data=unmatched.to_csv(index=False).encode("utf-8-sig"),
        file_name="anomalies_need_new_tasks.csv",
        mime="text/csv"
    )

st.markdown("---")
st.caption("MMS â€” Cross-Check: ÙŠØ·Ø§Ø¨Ù‚ Ù…Ù„Ù Ø§Ù„Ø´Ø°ÙˆØ° (Meter Number) Ù…Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØªØ³Ø±ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ ÙØ§Ù‚Ø¯.")
