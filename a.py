# -*- coding: utf-8 -*-
# meter_maintenance_ops_dashboard.py
import re
import io
import math
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime, date

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
# -------------------------------
st.set_page_config(page_title="Ù„ÙˆØ­Ø© ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª - Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙˆÙ…ØªØ§Ø¨Ø¹Ø©", layout="wide")

st.title("ğŸ“Š Ù„ÙˆØ­Ø© ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª")
st.caption("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ÙˆÙ…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© (ØªØ£Ø®ÙŠØ±ØŒ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±ÙƒÙŠÙ†).")

# -------------------------------
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# -------------------------------
def norm_col(c: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© (Ù„Ø§ ÙŠÙØ³ØªØ®Ø¯Ù… Ù„Ù„Ø¹Ø±Ø¶)."""
    return re.sub(r"\s+", " ", str(c).strip()).lower()

# Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© (Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
CLOSED_CANDIDATES = [
    "Task Closed Time", "Task Completed Time",
    "ÙˆÙ‚Øª Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©"
]
REG_CANDIDATES = [
    "Task Registration Date Time", "Request Registration Date Time",
    "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨"
]
STATUS_CANDIDATES = [
    "Task Status", "Request Status", "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©", "Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨"
]
METER_CANDIDATES = [
    "Meter No", "Meter Number", "Ø±Ù‚Ù… Ø§Ù„Ø¹Ø¯Ø§Ø¯"
]
TECH_CANDIDATES = [
    "Technician Name", "Field Team Name", "Ø§Ø³Ù… Ø§Ù„ÙÙ†ÙŠ", "Ø§Ø³Ù… ÙØ±ÙŠÙ‚ Ø§Ù„Ù…ÙŠØ¯Ø§Ù†"
]
REQ_TYPE_CANDIDATES = [
    "Request Type", "Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨"
]
REQ_CHANNEL_CANDIDATES = [
    "Request Channel", "Ù‚Ù†Ø§Ø© Ø§Ù„Ø·Ù„Ø¨", "Ù…ØµØ¯Ø± Ø§Ù„Ø·Ù„Ø¨"
]
VIP_CANDIDATES = [
    "Subscription VIP", "VIP", "Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ", "Account Type", "ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´ØªØ±Ùƒ"
]
BUCKET_HINTS = {
    "Ø§Ø³ØªØ¨Ø¯Ø§Ù„": ["Ø§Ø³ØªØ¨Ø¯Ø§Ù„"],
    "ØªØ­Ø³ÙŠÙ†": ["ØªØ­Ø³ÙŠÙ†", "Ø§Ø³ØªØ®Ø±Ø§Ø¬", "ØªØ­Ø³ÙŠÙ† ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬"],
    "ØµÙŠØ§Ù†Ø©": ["ØµÙŠØ§Ù†Ø©"],
    "ÙƒØ´Ù": ["ÙƒØ´Ù", "Ù…Ø¹Ø§ÙŠÙ†Ø©", "ÙƒØ´Ù ÙˆÙ…Ø¹Ø§ÙŠÙ†Ø©"]
}

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
    norm_map = {norm_col(c): c for c in df.columns}
    for c in candidates:
        nc = norm_col(c)
        if nc in norm_map:
            return norm_map[nc]
    # ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ Ø¥Ù† Ù„Ø²Ù… (Ø§Ø­ØªÙŠØ§Ø·)
    for c in df.columns:
        for cand in candidates:
            if norm_col(cand) in norm_col(c):
                return c
    return None

def infer_bucket_from_filename(name: str) -> str:
    n = (name or "").lower()
    for bucket, kws in BUCKET_HINTS.items():
        for kw in kws:
            if kw in n:
                return bucket
    return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

def read_one_excel(uploaded_file) -> tuple[pd.DataFrame, dict]:
    xl = pd.ExcelFile(uploaded_file)
    sheet = xl.sheet_names[0]
    df = xl.parse(sheet)

    # Ø£Ø¹Ù…Ø¯Ø© Ø£Ø³Ø§Ø³ÙŠØ©
    closed_col = pick_col(df, CLOSED_CANDIDATES)
    reg_col    = pick_col(df, REG_CANDIDATES)
    status_col = pick_col(df, STATUS_CANDIDATES)
    meter_col  = pick_col(df, METER_CANDIDATES)
    tech_col   = pick_col(df, TECH_CANDIDATES)
    type_col   = pick_col(df, REQ_TYPE_CANDIDATES)
    chan_col   = pick_col(df, REQ_CHANNEL_CANDIDATES)
    vip_col    = pick_col(df, VIP_CANDIDATES)

    # ØªÙˆØ§Ø±ÙŠØ®
    if closed_col is not None:
        df[closed_col] = pd.to_datetime(df[closed_col], errors="coerce")
        df["Closed Date"] = df[closed_col].dt.date
    else:
        df["Closed Date"] = pd.NaT

    if reg_col is not None:
        df[reg_col] = pd.to_datetime(df[reg_col], errors="coerce")

    # Ø§Ù„Ø³Ù„Ø©
    if "Bucket" not in df.columns:
        df["Bucket"] = infer_bucket_from_filename(getattr(uploaded_file, "name", ""))

    # Ù…Ø¯Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² (Ù„Ù„Ù…Ù†Ø¬Ø²Ø©)
    if (closed_col is not None) and (reg_col is not None):
        df["Duration Hours"] = (df[closed_col] - df[reg_col]).dt.total_seconds() / 3600.0

    meta = dict(
        sheet=sheet, closed_col=closed_col, reg_col=reg_col, status_col=status_col,
        meter_col=meter_col, tech_col=tech_col, type_col=type_col,
        chan_col=chan_col, vip_col=vip_col, columns=list(df.columns)
    )
    return df, meta

def compare_columns(files_meta: dict[str, dict]) -> pd.DataFrame:
    all_cols = []
    for name, info in files_meta.items():
        all_cols.extend(info["columns"])
    ordered = list(dict.fromkeys(all_cols))  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
    out = pd.DataFrame({"Column": ordered})
    for name, info in files_meta.items():
        cols = set(info["columns"])
        out[name] = out["Column"].apply(lambda c: c in cols)
    return out

def today_date() -> date:
    return datetime.now().date()

# -------------------------------
# Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ: Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª + Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# -------------------------------
with st.sidebar:
    st.header("ğŸ“ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
    files = st.file_uploader(
        "Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Excel (ÙŠÙ…ÙƒÙ† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª)",
        type=["xlsx", "xls"], accept_multiple_files=True
    )
    st.markdown("---")
    st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    sla_days = st.number_input("Ø­Ø¯ Ø§Ù„ØªØ£Ø®ÙŠØ± (SLA) Ø¨Ø§Ù„Ø£ÙŠØ§Ù…", min_value=1, max_value=60, value=3)
    overdue_buckets = st.multiselect(
        "ØªÙ‚Ø³ÙŠÙ… ÙØªØ±Ø§Øª Ø§Ù„ØªØ£Ø®ÙŠØ± (Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©)",
        ["â‰¤1 ÙŠÙˆÙ…", "2-3 Ø£ÙŠØ§Ù…", "4-7 Ø£ÙŠØ§Ù…", "8-30 ÙŠÙˆÙ…Ù‹Ø§", ">30 ÙŠÙˆÙ…"],
        default=["2-3 Ø£ÙŠØ§Ù…", "4-7 Ø£ÙŠØ§Ù…", "8-30 ÙŠÙˆÙ…Ù‹Ø§", ">30 ÙŠÙˆÙ…"]
    )
    st.markdown("---")
    st.caption("ØªÙØ³ØªÙ†ØªØ¬ Ø§Ù„Ø³Ù„Ø© Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙˆØ¯ Bucket.")

# -------------------------------
# ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# -------------------------------
dfs, metas = [], {}
if files:
    for f in files:
        df, meta = read_one_excel(f)
        dfs.append(df)
        metas[f.name] = meta

if not files:
    st.info("âœ¨ Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø³Ù„Ø§Ù„ (Ø§Ø³ØªØ¨Ø¯Ø§Ù„/ØµÙŠØ§Ù†Ø©/ØªØ­Ø³ÙŠÙ†/ÙƒØ´Ù) Ù„Ù„Ø¨Ø¯Ø¡.")
    st.stop()

data = pd.concat(dfs, ignore_index=True, sort=False)

# Ø¹Ø±Ø¶ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
with st.expander("ğŸ” Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª"):
    comp = compare_columns(metas)
    st.dataframe(comp, use_container_width=True)

# Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
any_meta = next(iter(metas.values()))
closed_col = next((m["closed_col"] for m in metas.values() if m["closed_col"]), None)
reg_col    = next((m["reg_col"]    for m in metas.values() if m["reg_col"]), None)
status_col = next((m["status_col"] for m in metas.values() if m["status_col"]), None)
meter_col  = next((m["meter_col"]  for m in metas.values() if m["meter_col"]), None)
tech_col   = next((m["tech_col"]   for m in metas.values() if m["tech_col"]), None)
type_col   = next((m["type_col"]   for m in metas.values() if m["type_col"]), None)
chan_col   = next((m["chan_col"]   for m in metas.values() if m["chan_col"]), None)
vip_col    = next((m["vip_col"]    for m in metas.values() if m["vip_col"]), None)

# -------------------------------
# ÙÙ„Ø§ØªØ± Ø¹Ø§Ù…Ø©
# -------------------------------
st.markdown("## ğŸ¯ Ø§Ù„ÙÙ„Ø§ØªØ± Ø§Ù„Ø¹Ø§Ù…Ø©")
colf1, colf2, colf3 = st.columns([1,1,2])

# ÙÙ„ØªØ± Ø§Ù„ØªØ§Ø±ÙŠØ® (Ù„Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©): Ø­Ø³Ø¨ Closed Date
if "Closed Date" in data.columns and data["Closed Date"].notna().any():
    min_d = pd.to_datetime(data["Closed Date"]).min().date()
    max_d = pd.to_datetime(data["Closed Date"]).max().date()
    with colf1:
        date_mode = st.radio("Ù†Ø·Ø§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©)", ["ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯", "Ù†Ø·Ø§Ù‚ Ø£ÙŠØ§Ù…"], horizontal=True)
    with colf2:
        if date_mode == "ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯":
            sel_day = st.date_input("Ø§Ø®ØªØ± Ø§Ù„ÙŠÙˆÙ…", value=max_d, min_value=min_d, max_value=max_d)
            prod_mask = (pd.to_datetime(data["Closed Date"]).dt.date == sel_day)
        else:
            sel_range = st.date_input("Ø§Ø®ØªØ± Ø§Ù„Ù†Ø·Ø§Ù‚", value=(min_d, max_d), min_value=min_d, max_value=max_d)
            d1, d2 = sel_range
            prod_mask = (pd.to_datetime(data["Closed Date"]).dt.date >= d1) & (pd.to_datetime(data["Closed Date"]).dt.date <= d2)
else:
    st.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Closed Date ØµØ§Ù„Ø­. Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø¨Ø¯ÙˆÙ† ÙÙ„ØªØ± Ø²Ù…Ù†ÙŠ.")
    prod_mask = np.array([True] * len(data))

with colf3:
    buckets = sorted(data["Bucket"].dropna().astype(str).unique().tolist()) if "Bucket" in data.columns else []
    sel_buckets = st.multiselect("ØªØµÙÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø©", buckets, default=buckets)

mask_bucket = data["Bucket"].astype(str).isin(sel_buckets) if "Bucket" in data.columns and sel_buckets else np.array([True]*len(data))
data_filt_prod = data[prod_mask & mask_bucket].copy()

# -------------------------------
# 1) Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© (Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø§Ù„)
# -------------------------------
st.markdown("## ğŸ“ˆ Ø£ÙˆÙ„Ù‹Ø§: Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© Ù„ÙƒÙ„ Ø§Ù„Ø³Ù„Ø§Ù„")

# KPIs
total_tasks = len(data_filt_prod)
closed_tasks = data_filt_prod["Closed Date"].notna().sum() if "Closed Date" in data_filt_prod.columns else 0
closure_rate = (closed_tasks / total_tasks * 100) if total_tasks else 0
avg_duration = data_filt_prod["Duration Hours"].mean() if "Duration Hours" in data_filt_prod.columns else np.nan

k1, k2, k3, k4 = st.columns(4)
k1.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… (Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚)", f"{total_tasks:,}")
k2.metric("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‚ÙÙ„Ø©", f"{closed_tasks:,}")
k3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ù‚ÙØ§Ù„", f"{closure_rate:,.1f}%")
k4.metric("Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² (Ø³Ø§Ø¹Ø©)", "-" if (math.isnan(avg_duration) if isinstance(avg_duration, float) else pd.isna(avg_duration)) else f"{avg_duration:,.1f}")

# Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ…/Ø§Ù„Ø³Ù„Ø©
if "Closed Date" in data_filt_prod.columns:
    prod_group = data_filt_prod.groupby(["Closed Date", "Bucket"]).size().reset_index(name="Count")
    st.markdown("#### Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‚ÙÙ„Ø© ÙŠÙˆÙ…ÙŠÙ‹Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø©")
    st.bar_chart(prod_group.pivot(index="Closed Date", columns="Bucket", values="Count").fillna(0))

# ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø©
if "Bucket" in data_filt_prod.columns:
    st.markdown("#### ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø© (Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚)")
    st.bar_chart(data_filt_prod.groupby("Bucket").size())

# Ø£ÙØ¶Ù„ ÙÙ†ÙŠÙŠÙ† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
if tech_col and tech_col in data_filt_prod.columns:
    top_tech = (
        data_filt_prod.groupby(tech_col).size().reset_index(name="Count")
        .sort_values("Count", ascending=False).head(10)
    )
    st.markdown("#### Ø£Ø¹Ù„Ù‰ 10 ÙÙ†ÙŠÙŠÙ† (Ø¹Ø¯Ø¯ Ù…Ù‡Ø§Ù… Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚)")
    st.bar_chart(top_tech.set_index(tech_col))

st.markdown("---")

# -------------------------------
# 2) Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§
# -------------------------------
st.markdown("## ğŸ§­ Ø«Ø§Ù†ÙŠÙ‹Ø§: Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§")

# ØªØ¹Ø±ÙŠÙ "Ù…ÙØªÙˆØ­Ø©" Ùˆ "Ù…ØªØ£Ø®Ø±Ø©"
is_closed = data[closed_col].notna() if closed_col else data["Closed Date"].notna()
is_open = ~is_closed

# Ø¹Ù…Ø± Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø¨Ø§Ù„Ø£ÙŠØ§Ù… (Ø§Ù„ÙŠÙˆÙ… - ØªØ³Ø¬ÙŠÙ„)
if reg_col and reg_col in data.columns:
    data["Age Days"] = (pd.to_datetime(datetime.now()) - data[reg_col]).dt.days
else:
    data["Age Days"] = np.nan

open_df = data[is_open].copy()
open_df["Is Late"] = False
if "Age Days" in open_df.columns:
    open_df["Is Late"] = open_df["Age Days"] > sla_days

# KPIs Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
o1, o2, o3, o4 = st.columns(4)
o1.metric("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©", f"{len(open_df):,}")
o2.metric("Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ£Ø®Ø±Ø© (>{} ÙŠÙˆÙ…)".format(sla_days), f"{open_df['Is Late'].sum():,}")
if "Bucket" in open_df.columns:
    late_rate = (open_df["Is Late"].mean() * 100) if len(open_df) else 0
else:
    late_rate = 0
o3.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ£Ø®ÙŠØ±", f"{late_rate:,.1f}%")
if reg_col and reg_col in open_df.columns:
    avg_age = open_df["Age Days"].mean()
else:
    avg_age = np.nan
o4.metric("Ù…ØªÙˆØ³Ø· Ø¹Ù…Ø± Ø§Ù„Ù…Ù‡Ù…Ø© (ÙŠÙˆÙ…)", "-" if (math.isnan(avg_age) if isinstance(avg_age, float) else pd.isna(avg_age)) else f"{avg_age:,.1f}")

# ØªÙ‚Ø³ÙŠÙ… ÙØªØ±Ø§Øª Ø§Ù„ØªØ£Ø®ÙŠØ± Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©
def overdue_bucket(d):
    if pd.isna(d):
        return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    if d <= 1: return "â‰¤1 ÙŠÙˆÙ…"
    if d <= 3: return "2-3 Ø£ÙŠØ§Ù…"
    if d <= 7: return "4-7 Ø£ÙŠØ§Ù…"
    if d <= 30: return "8-30 ÙŠÙˆÙ…Ù‹Ø§"
    return ">30 ÙŠÙˆÙ…"

if "Age Days" in open_df.columns:
    open_df["Delay Bucket"] = open_df["Age Days"].apply(overdue_bucket)
    st.markdown("#### ØªÙˆØ²ÙŠØ¹ ÙØªØ±Ø§Øª Ø§Ù„ØªØ£Ø®ÙŠØ± Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©")
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    if overdue_buckets:
        filt_over = open_df["Delay Bucket"].isin(overdue_buckets)
        open_for_buckets = open_df[filt_over]
    else:
        open_for_buckets = open_df
    st.bar_chart(open_for_buckets.groupby("Delay Bucket").size())

# ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø©/Ø§Ù„ÙÙ†ÙŠ/Ø§Ù„Ù‚Ù†Ø§Ø©/Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨
cols = st.columns(3)
if "Bucket" in open_df.columns:
    with cols[0]:
        st.markdown("#### Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø­Ø³Ø¨ Ø§Ù„Ø³Ù„Ø©")
        st.bar_chart(open_df.groupby("Bucket").size())
if tech_col and tech_col in open_df.columns:
    with cols[1]:
        st.markdown("#### Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø­Ø³Ø¨ Ø§Ù„ÙÙ†ÙŠ (Top 10)")
        t = (open_df.groupby(tech_col).size().reset_index(name="Count")
             .sort_values("Count", ascending=False).head(10))
        st.bar_chart(t.set_index(tech_col))
if chan_col and chan_col in open_df.columns:
    with cols[2]:
        st.markdown("#### Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© Ø­Ø³Ø¨ Ù…ØµØ¯Ø± Ø§Ù„Ø·Ù„Ø¨")
        st.bar_chart(open_df.groupby(chan_col).size())

# Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø°Ø§Øª Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ù‡Ù…Ø© (Ø£ÙŠÙ‹Ø§ ÙƒØ§Ù†Øª Ø­Ø§Ù„ØªÙ‡Ø§)
if meter_col and meter_col in data.columns:
    multi_meter = (data.groupby(meter_col).size()
                   .reset_index(name="TasksCount")
                   .sort_values("TasksCount", ascending=False))
    st.markdown("#### Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
    st.dataframe(multi_meter[multi_meter["TasksCount"] > 1].head(200), use_container_width=True)

# ØªØ¹Ø¯Ø¯ Ø§Ù„Ø³Ù„Ø§Ù„ Ù„Ù†ÙØ³ Ø§Ù„Ø¹Ø¯Ø§Ø¯
if meter_col and "Bucket" in data.columns:
    meter_bucket_multi = (data.groupby([meter_col, "Bucket"]).size()
                          .reset_index(name="Count"))
    st.markdown("#### ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ù„ÙƒÙ„ Ø¹Ø¯Ø§Ø¯ Ø¹Ø¨Ø± Ø§Ù„Ø³Ù„Ø§Ù„")
    st.dataframe(meter_bucket_multi.sort_values(["Count"], ascending=False).head(300), use_container_width=True)

# ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø·Ù„Ø¨ ÙˆØ§Ù„Ù…ØªØ¨Ù‚ÙŠ
if type_col and type_col in open_df.columns:
    st.markdown("#### Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø·Ù„Ø¨ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø©")
    st.bar_chart(open_df.groupby(type_col).size())

# VIP / Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ
if vip_col and vip_col in data.columns:
    st.markdown("#### Ù…ØªØ§Ø¨Ø¹Ø© VIP / Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ")
    vip_open = (open_df.groupby(vip_col).size().reset_index(name="OpenCount")
                .sort_values("OpenCount", ascending=False))
    st.dataframe(vip_open, use_container_width=True)
    if "Is Late" in open_df.columns:
        vip_late = (open_df.groupby(vip_col)["Is Late"].mean().reset_index())
        vip_late["Late %"] = (vip_late["Is Late"] * 100).round(1)
        vip_late = vip_late.drop(columns=["Is Late"])
        st.markdown("**Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ£Ø®ÙŠØ± Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØªØ±Ùƒ**")
        st.dataframe(vip_late, use_container_width=True)

# Ø¬Ø¯ÙˆÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© (Ø§Ù„Ø£Ù‡Ù… ØªØ´ØºÙŠÙ„ÙŠÙ‹Ø§)
st.markdown("### ğŸ“‹ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ØªØ¹Ø§Ù…Ù„)")
show_cols = []
for c in ["Bucket", "Closed Date", "Duration Hours", "Age Days", "Delay Bucket",
          "Task Code", "Request id", "Request Type", "Task Status",
          "Task Registration Date Time", "Request Registration Date Time",
          "Task Closed Time", "Task Completed Time"]:
    if c in open_df.columns:
        show_cols.append(c)
# ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ„ Ù…Ù‡Ù…Ø© Ø¹Ù†Ø¯ ØªÙˆÙØ±Ù‡Ø§
for c in [meter_col, tech_col, chan_col, vip_col]:
    if c and c in open_df.columns:
        show_cols.append(c)
show_cols = list(dict.fromkeys(show_cols))  # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±
st.dataframe(open_df.sort_values("Age Days", ascending=False)[show_cols] if show_cols else open_df, use_container_width=True)

# ØªÙ†Ø²ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª
st.markdown("---")
col_dl1, col_dl2 = st.columns(2)
with col_dl1:
    csv_prod = data_filt_prod.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© (CSV)", data=csv_prod, file_name="productivity_filtered.csv", mime="text/csv")
with col_dl2:
    csv_open = open_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙØªÙˆØ­Ø© (CSV)", data=csv_open, file_name="open_tasks.csv", mime="text/csv")

st.markdown("---")
st.caption("Â© Ù„ÙˆØ­Ø© ØµÙŠØ§Ù†Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª â€” Ø¥Ù†ØªØ§Ø¬ÙŠØ© ÙŠÙˆÙ…ÙŠØ© + Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ (ØªØ£Ø®ÙŠØ±ØŒ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§ØªØŒ VIP).")
