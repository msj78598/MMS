# -*- coding: utf-8 -*-
# mms_premise_tracker.py â€” Premise key + robust LastDaily + safe open/closed + Excel export

import re
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime
from io import BytesIO

st.set_page_config(page_title="MMS | Premise Tracker", layout="wide")

# ===================== Helpers =====================
def norm_col(c: str) -> str:
    return re.sub(r"\s+", " ", str(c).strip()).lower()

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
    """ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¨Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ø«Ù… Ø¨Ø§Ù„Ø§Ø­ØªÙˆØ§Ø¡ Ø§Ù„Ø¬Ø²Ø¦ÙŠ (AR/EN)."""
    if df is None or df.empty:
        return None
    nm = {norm_col(c): c for c in df.columns}
    for c in candidates:
        if norm_col(c) in nm:
            return nm[norm_col(c)]
    for c in df.columns:
        for cand in candidates:
            if norm_col(cand) in norm_col(c):
                return c
    return None

def smart_parse_datetime(series: pd.Series, excel_origin: str = "1899-12-30") -> pd.Series:
    """ØªØ­ÙˆÙŠÙ„ Ù…Ø®ØªÙ„Ø· (Ù†ØµÙŠ/Ø³ÙŠØ±ÙŠØ§Ù„ Excel) Ø¥Ù„Ù‰ datetimeØ› ÙŠØ¯Ø¹Ù… dayfirst ÙˆExcel 1900/1904."""
    if series is None:
        return pd.Series([], dtype="datetime64[ns]")

    s = series.copy()

    def clean(x):
        if pd.isna(x): return np.nan
        x = str(x).strip()
        if x == "" or x.lower() in {"none", "nan", "null", "-", "â€”", "0"}: return np.nan
        # Ù…Ø«Ù„ 0000-00-00
        if re.fullmatch(r"0{2,}[-/:]0{2,}[-/:]0{2,}.*", x): return np.nan
        return x

    s = s.map(clean)

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£ÙˆÙ„Ù‰ (dayfirst=True)
    parsed = pd.to_datetime(s, errors="coerce", dayfirst=True, infer_datetime_format=True)

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø«Ø§Ù†ÙŠØ© (dayfirst=False) Ù„Ù…Ø§ ØªÙØ´Ù„ Ø§Ù„Ø£ÙˆÙ„Ù‰
    need2 = parsed.isna()
    if need2.any():
        parsed.loc[need2] = pd.to_datetime(s[need2], errors="coerce", dayfirst=False, infer_datetime_format=True)

    # Excel serial fallback (1900/1904 origin)
    need_excel = parsed.isna()
    if need_excel.any():
        as_num = pd.to_numeric(s.where(need_excel), errors="coerce")
        mask = as_num.notna()
        if mask.any():
            parsed.loc[mask] = pd.to_datetime(as_num[mask], unit="d", origin=excel_origin, errors="coerce")

    return parsed

def as_int0(x):
    """ØªØ­ÙˆÙŠÙ„ Ø¢Ù…Ù† Ø¥Ù„Ù‰ intØ› ÙŠØ¹ÙŠØ¯ 0 Ø¹Ù†Ø¯ NaN/None/ØºÙŠØ± Ø±Ù‚Ù…ÙŠ."""
    try:
        v = pd.to_numeric(x, errors="coerce")
        if hasattr(v, "iloc"):
            v = v.iloc[0]
        if pd.isna(v):
            return 0
        return int(float(v))
    except Exception:
        return 0

def to_excel_download(df: pd.DataFrame) -> bytes:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ExcelØ› ÙŠÙØ¶Ù‘Ù„ xlsxwriter ÙˆØ¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ ÙŠØ³ØªØ®Ø¯Ù… openpyxl."""
    bio = BytesIO()
    try:
        with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Results")
    except ModuleNotFoundError:
        with pd.ExcelWriter(bio, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Results")
    return bio.getvalue()

# ===================== UI: Uploads & Settings =====================
st.title("ğŸ“Š Ù†Ø¸Ø§Ù… ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø© â€” Premise Tracker")

with st.sidebar:
    st.header("ğŸ“ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„")
    disconnected_file = st.file_uploader("Ù…Ù„Ù Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø©", type=["xlsx", "xls"])
    insp_files  = st.file_uploader("Ù…Ù„ÙØ§Øª Ù…Ù‡Ø§Ù… Ø§Ù„ÙØ­Øµ (0..N)",  type=["xlsx", "xls"], accept_multiple_files=True)
    maint_files = st.file_uploader("Ù…Ù„ÙØ§Øª Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø© (0..N)", type=["xlsx", "xls"], accept_multiple_files=True)

    st.markdown("---")
    st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­ÙˆÙŠÙ„")
    excel_origin = st.selectbox("Excel Origin (Ù„Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø±Ù‚Ù…ÙŠØ©)", ["1899-12-30", "1904-01-01"], index=0)

    st.markdown("â€”")
    st.header("ğŸ”’ Ø­Ø§Ù„Ø§Øª ØªØ¹ØªØ¨Ø± (Ù…Ù‚ÙÙ„Ø©)")
    default_closed_terms = """
closed, complete, completed, done, resolved,
Ù…ØºÙ„Ù‚, Ù…ØºÙ„Ù‚Ø©, Ù…Ù‚ÙÙ„Ø©, Ù…Ù‚ÙÙ„, Ù…Ù†Ø¬Ø², Ù…Ù†Ø¬Ø²Ø©, Ù…Ù†ØªÙ‡ÙŠØ©, Ù…Ù†ØªÙ‡ÙŠ, ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
""".strip()
    closed_terms_input = st.text_area("Ù‚Ø§Ø¦Ù…Ø© Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„ (ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§)", value=default_closed_terms, height=90)
    CLOSED_TERMS = {w.strip().lower() for w in closed_terms_input.split(",") if w.strip()}

    st.markdown("---")
    start_btn = st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„")

if not start_btn or not disconnected_file:
    st.info("â¬†ï¸ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø«Ù… Ø§Ø¶ØºØ· **Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„**.")
    st.stop()

# ===================== Read Disconnected (Premise key) =====================
st.subheader("ğŸ“˜ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø© (Premise = Ø§Ù„Ù…ÙØªØ§Ø­)")

dis_df = pd.read_excel(disconnected_file)

PREMISE_CANDS_DIS = ["Utility Site Id", "Premise", "Ø±Ù‚Ù… Ø§Ù„Ù…ÙƒØ§Ù†"]
LAST_CANDS = [
    "Last Daily", "LastDaily", "Last Communication", "Last Comm",
    "Ø¢Ø®Ø± Ù‚Ø±Ø§Ø¡Ø©", "Ø¢Ø®Ø± Ø§ØªØµØ§Ù„", "Ø§Ø®Ø± Ø§ØªØµØ§Ù„", "Ø§Ø®Ø± Ù‚Ø±Ø§Ø¡Ø©", "Last Daily Read", "Last Daily Date"
]

premise_col_dis = pick_col(dis_df, PREMISE_CANDS_DIS)
last_daily_col  = pick_col(dis_df, LAST_CANDS)

if not premise_col_dis:
    st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù… Ø§Ù„Ù…ÙƒØ§Ù† ÙÙŠ Ù…Ù„Ù ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„ÙŠÙ† (ØªÙˆÙ‚Ù‘Ø¹: Utility Site Id).")
    st.stop()

dis_df["_KEY_PREMISE"] = dis_df[premise_col_dis].astype(str).str.strip()

# Ø§Ø®ØªÙŠØ§Ø± ÙŠØ¯ÙˆÙŠ Ù„Ø¹Ù…ÙˆØ¯ LastDaily (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
with st.expander("ğŸ”§ Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„' ÙŠØ¯ÙˆÙŠÙ‹Ø§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)"):
    last_choice = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„' (Ø¥Ù† Ø±ØºØ¨Øª):", options=["(Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ)"] + list(dis_df.columns), index=0)
    if last_choice != "(Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ)":
        last_daily_col = last_choice

# ØªØ­ÙˆÙŠÙ„ LastDaily
if last_daily_col and last_daily_col in dis_df.columns:
    parsed = smart_parse_datetime(dis_df[last_daily_col], excel_origin=excel_origin)
    dis_df["LastDaily"] = parsed
    ok = int(parsed.notna().sum())
    st.success(f"ØªØ­ÙˆÙŠÙ„ '{last_daily_col}': {ok}/{len(dis_df)} Ù‚ÙŠÙ…Ø§Ù‹ ØµØ§Ù„Ø­Ø©.")
    with st.expander("ğŸ§ª Ø£Ù…Ø«Ù„Ø© ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„"):
        bad = dis_df.loc[parsed.isna(), [last_daily_col]].head(15)
        st.write("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ù…Ø«Ù„Ø© ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„ âœ…" if bad.empty else bad)
else:
    dis_df["LastDaily"] = pd.NaT
    st.warning("âš ï¸ Ù„Ù… ÙŠÙØ¹Ø«Ø± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„' â€” Ø³ÙŠÙØªØ±Ùƒ ÙØ§Ø±ØºÙ‹Ø§.")

summary_base_cols = ["_KEY_PREMISE", "LastDaily"]
summary_extra_cols = [c for c in dis_df.columns if c not in summary_base_cols]
summary = dis_df[summary_base_cols + summary_extra_cols].copy()

# ===================== Read Tasks (multi files) =====================
def load_task_files(files, kind_label: str) -> pd.DataFrame:
    """ØªÙˆØ­ÙŠØ¯ Ù…Ø®Ø·Ø· Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ­Øµ/Ø§Ù„ØµÙŠØ§Ù†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Premise ÙƒÙ…ÙØªØ§Ø­."""
    if not files:
        return pd.DataFrame()

    frames = []
    for f in files:
        df = pd.read_excel(f)

        premise_col = pick_col(df, ["Premise", "Utility Site Id", "Ø±Ù‚Ù… Ø§Ù„Ù…ÙƒØ§Ù†"])
        reg_col     = pick_col(df, ["Task Registration Date Time", "Request Registration Date Time",
                                    "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨"])
        close_col   = pick_col(df, ["Task Closed Time", "Task Completed Time",
                                    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù‚ÙØ§Ù„", "ÙˆÙ‚Øª Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©"])
        status_col  = pick_col(df, ["Task Status", "Request Status", "Ø§Ù„Ø­Ø§Ù„Ø©",
                                    "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©", "Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨"])
        result_col  = pick_col(df, ["Technician's Result", "Final Result",
                                    "Final Result (Dispatcher's Result)", "Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©", "Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙÙ†ÙŠ"])

        if not premise_col:
            st.warning(f"ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„Ù '{getattr(f,'name','file')}' Ù„Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Premise/Utility Site Id.")
            continue

        tmp = pd.DataFrame()
        tmp["_KEY_PREMISE"] = df[premise_col].astype(str).str.strip()
        tmp["reg_time"]   = smart_parse_datetime(df[reg_col],   excel_origin=excel_origin) if (reg_col   and reg_col   in df.columns) else pd.NaT
        tmp["close_time"] = smart_parse_datetime(df[close_col], excel_origin=excel_origin) if (close_col and close_col in df.columns) else pd.NaT
        tmp["status"]     = df[status_col].astype(str)  if (status_col and status_col in df.columns) else np.nan
        tmp["result"]     = df[result_col].astype(str)  if (result_col and result_col in df.columns) else np.nan
        tmp["bucket"]     = kind_label
        tmp["source"]     = getattr(f, "name", kind_label)
        frames.append(tmp)

    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True, sort=False)

insp_df  = load_task_files(insp_files,  "ÙØ­Øµ")
maint_df = load_task_files(maint_files, "ØµÙŠØ§Ù†Ø©")

# ===================== Safe Summaries =====================
def summarize_tasks(df: pd.DataFrame, prefix: str, closed_terms: set[str]) -> pd.DataFrame:
    """ØªØ¬Ù…ÙŠØ¹ Ø¢Ù…Ù† Ù„ÙƒÙ„ Premise: Ø¥Ø¬Ù…Ø§Ù„ÙŠØŒ Ù…ÙØªÙˆØ­ØŒ Ø¢Ø®Ø± Ø­Ø§Ù„Ø©/Ù†ØªÙŠØ¬Ø©/ØªØ§Ø±ÙŠØ®."""
    if df is None or df.empty or "_KEY_PREMISE" not in df.columns:
        return pd.DataFrame(columns=["_KEY_PREMISE"])

    df = df.copy()
    for col in ["reg_time", "close_time", "status", "result"]:
        if col not in df.columns:
            df[col] = pd.NaT if col in ["reg_time", "close_time"] else np.nan

    status_norm = df["status"].astype(str).str.strip().str.lower()
    is_closed_by_status = status_norm.isin(closed_terms)
    is_closed_by_time   = df["close_time"].notna()
    df["_is_open"] = ~(is_closed_by_status | is_closed_by_time)

    df["_latest_date"] = df["close_time"].where(df["close_time"].notna(), df["reg_time"])

    latest = (
        df.sort_values(["_KEY_PREMISE", "_latest_date", "reg_time"], na_position="last")
          .drop_duplicates("_KEY_PREMISE", keep="last")
          .loc[:, ["_KEY_PREMISE", "status", "result", "_latest_date"]]
          .rename(columns={
              "status": f"{prefix}latest_status",
              "result": f"{prefix}latest_result",
              "_latest_date": f"{prefix}latest_date"
          })
    )

    agg = (
        df.groupby("_KEY_PREMISE", dropna=False)
          .agg(**{
              f"{prefix}total": ("_KEY_PREMISE", "count"),
              f"{prefix}open":  ("_is_open", "sum"),
          })
          .reset_index()
    )

    out = agg.merge(latest, on="_KEY_PREMISE", how="left")
    return out

insp_sum  = summarize_tasks(insp_df,  "insp_",  CLOSED_TERMS)
maint_sum = summarize_tasks(maint_df, "maint_", CLOSED_TERMS)

summary = summary.merge(insp_sum,  on="_KEY_PREMISE", how="left")
summary = summary.merge(maint_sum, on="_KEY_PREMISE", how="left")

# ØªØ£ÙƒÙŠØ¯ Ù†ÙˆØ¹ LastDaily Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ù…Ø¬
if "LastDaily" in summary.columns:
    summary["LastDaily"] = smart_parse_datetime(summary["LastDaily"], excel_origin=excel_origin)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù„Ø£Ø¹Ø¯Ø§Ø¯ ØµØ­ÙŠØ­Ø©
for col in ["insp_open", "maint_open", "insp_total", "maint_total"]:
    if col in summary.columns:
        summary[col] = pd.to_numeric(summary[col], errors="coerce").fillna(0).astype(int)

# ===================== Next Action =====================
def next_action(row):
    insp_open  = as_int0(row.get("insp_open", 0))
    maint_open = as_int0(row.get("maint_open", 0))
    if maint_open > 0:
        return "ØªØ³Ø±ÙŠØ¹ ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø©"
    if insp_open > 0:
        return "Ù…ØªØ§Ø¨Ø¹Ø© ÙØ­Øµ Ù…ÙØªÙˆØ­"
    return "Ù„Ø§ Ø¥Ø¬Ø±Ø§Ø¡"

summary["Next Action"] = summary.apply(next_action, axis=1)

# ===================== KPIs =====================
st.subheader("ğŸ“ˆ Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ø§Ù…Ø©")
c1, c2, c3 = st.columns(3)
c1.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø©", f"{summary['_KEY_PREMISE'].nunique():,}")
c2.metric("Ù…Ù‡Ø§Ù… ÙØ­Øµ Ù…ÙØªÙˆØ­Ø©",        int(summary.get("insp_open",  pd.Series()).fillna(0).sum()))
c3.metric("Ù…Ù‡Ø§Ù… ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø©",       int(summary.get("maint_open", pd.Series()).fillna(0).sum()))

# ===================== Unified Table =====================
st.subheader("ğŸ“‹ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ÙˆØ­Ø¯")
display_cols = ["_KEY_PREMISE", "LastDaily"]
for c in ["Office", "States", "Logistic State", "Gateway Id", "Latitude", "Longitude"]:
    if c in summary.columns:
        display_cols.append(c)
display_cols += [c for c in [
    "insp_total","insp_open","insp_latest_status","insp_latest_result","insp_latest_date",
    "maint_total","maint_open","maint_latest_status","maint_latest_result","maint_latest_date",
    "Next Action"
] if c in summary.columns]

st.dataframe(summary[display_cols] if display_cols else summary, use_container_width=True)

# ===================== Download (Excel) =====================
excel_bytes = to_excel_download(summary)
st.download_button(
    label="â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Excel)",
    data=excel_bytes,
    file_name=f"premise_tracker_results_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
)
