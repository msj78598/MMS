# -*- coding: utf-8 -*-
# mms_inspection_impact.py â€” Ø¥Ø¨Ø±Ø§Ø² Ø¬Ù‡ÙˆØ¯ Ø§Ù„ÙØ­Øµ ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„Ø§ØªØµØ§Ù„ (Premise Key)

import re
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime
from io import BytesIO

st.set_page_config(page_title="Inspection Impact â€” Premise Tracker", layout="wide")

# ================= Helpers =================
def norm_col(c: str) -> str:
    return re.sub(r"\s+", " ", str(c).strip()).lower()

def pick_col(df: pd.DataFrame, candidates: list[str]) -> str | None:
    if df is None or df.empty:
        return None
    nm = {norm_col(c): c for c in df.columns}
    for c in candidates:
        if norm_col(c) in nm:
            return nm[norm_col(c)]
    for c in df.columns:
        for cand in candidates:
            if norm_col(cand) in norm_col(c):
                return c
    return None

def smart_parse_datetime(series: pd.Series, excel_origin: str = "1899-12-30") -> pd.Series:
    """ØªØ­ÙˆÙŠÙ„ Ù…Ø®ØªÙ„Ø· (Ù†ØµÙŠ/Ø³ÙŠØ±ÙŠØ§Ù„ Excel) Ø¥Ù„Ù‰ datetimeØ› ÙŠØ¯Ø¹Ù… dayfirst Ùˆ1900/1904."""
    if series is None:
        return pd.Series([], dtype="datetime64[ns]")
    s = series.copy()

    def clean(x):
        if pd.isna(x): return np.nan
        x = str(x).strip()
        if x == "" or x.lower() in {"none", "nan", "null", "-", "â€”", "0"}: return np.nan
        if re.fullmatch(r"0{2,}[-/:]0{2,}[-/:]0{2,}.*", x): return np.nan
        return x

    s = s.map(clean)
    parsed = pd.to_datetime(s, errors="coerce", dayfirst=True, infer_datetime_format=True)

    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø«Ø§Ù†ÙŠØ©
    need2 = parsed.isna()
    if need2.any():
        parsed.loc[need2] = pd.to_datetime(s[need2], errors="coerce", dayfirst=False, infer_datetime_format=True)

    # Excel serial
    need_excel = parsed.isna()
    if need_excel.any():
        as_num = pd.to_numeric(s.where(need_excel), errors="coerce")
        mask = as_num.notna()
        if mask.any():
            parsed.loc[mask] = pd.to_datetime(as_num[mask], unit="d", origin=excel_origin, errors="coerce")
    return parsed

def to_excel_download(df: pd.DataFrame) -> bytes:
    """Excel via xlsxwriter; fallback to openpyxl."""
    bio = BytesIO()
    try:
        with pd.ExcelWriter(bio, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Results")
    except ModuleNotFoundError:
        with pd.ExcelWriter(bio, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="Results")
    return bio.getvalue()

# ================= UI (Uploads & Settings) =================
st.title("ğŸ“Š Ø¥Ø¨Ø±Ø§Ø² Ø¬Ù‡ÙˆØ¯ Ø§Ù„ÙØ­Øµ â€” Inspection Impact (Premise Key)")

with st.sidebar:
    st.header("ğŸ“ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„")
    dis_file   = st.file_uploader("Ù…Ù„Ù Ø§Ù„Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø©", type=["xlsx","xls"])
    insp_files = st.file_uploader("Ù…Ù„ÙØ§Øª Ù…Ù‡Ø§Ù… Ø§Ù„ÙØ­Øµ (0..N)",  type=["xlsx","xls"], accept_multiple_files=True)
    mnt_files  = st.file_uploader("Ù…Ù„ÙØ§Øª Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø© (0..N)", type=["xlsx","xls"], accept_multiple_files=True)

    st.markdown("---")
    st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    excel_origin = st.selectbox("Excel Origin (Ù„Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø±Ù‚Ù…ÙŠØ©)", ["1899-12-30", "1904-01-01"], index=0)

    default_closed_terms = """
closed, complete, completed, done, resolved,
Ù…ØºÙ„Ù‚, Ù…ØºÙ„Ù‚Ø©, Ù…Ù‚ÙÙ„Ø©, Ù…Ù‚ÙÙ„, Ù…Ù†Ø¬Ø², Ù…Ù†Ø¬Ø²Ø©, Ù…Ù†ØªÙ‡ÙŠØ©, Ù…Ù†ØªÙ‡ÙŠ, ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
""".strip()
    closed_terms_input = st.text_area("Ø­Ø§Ù„Ø§Øª ØªØ¹ØªØ¨Ø± (Ù…Ù‚ÙÙ„Ø©) â€” Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„", value=default_closed_terms, height=90)
    CLOSED_TERMS = {w.strip().lower() for w in closed_terms_input.split(",") if w.strip()}

    st.markdown("---")
    start_btn = st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„")

if not start_btn or not dis_file:
    st.info("â¬†ï¸ Ø§Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø«Ù… Ø§Ø¶ØºØ· **Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù„ÙŠÙ„**.")
    st.stop()

# ================= Read Disconnected =================
dis_df = pd.read_excel(dis_file)

PREMISE_CANDS_DIS = ["Utility Site Id", "Premise", "Ø±Ù‚Ù… Ø§Ù„Ù…ÙƒØ§Ù†"]
LAST_CANDS = [
    "Last Daily", "LastDaily", "Last Communication", "Last Comm",
    "Last Daily Read", "Last Daily Date",
    "Ø¢Ø®Ø± Ù‚Ø±Ø§Ø¡Ø©", "Ø¢Ø®Ø± Ø§ØªØµØ§Ù„", "Ø§Ø®Ø± Ø§ØªØµØ§Ù„", "Ø§Ø®Ø± Ù‚Ø±Ø§Ø¡Ø©"
]

premise_dis = pick_col(dis_df, PREMISE_CANDS_DIS)
last_col    = pick_col(dis_df, LAST_CANDS)

if not premise_dis:
    st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ Premise/Utility Site Id ÙÙŠ Ù…Ù„Ù ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„ÙŠÙ†.")
    st.stop()

dis_df["_KEY_PREMISE"] = dis_df[premise_dis].astype(str).str.strip()

with st.expander("ğŸ”§ Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„' ÙŠØ¯ÙˆÙŠÙ‹Ø§ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)"):
    last_choice = st.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„':", options=["(Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ)"] + list(dis_df.columns), index=0)
    if last_choice != "(Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ)":
        last_col = last_choice

# LastDaily
if last_col and last_col in dis_df.columns:
    dis_df["LastDaily"] = smart_parse_datetime(dis_df[last_col], excel_origin=excel_origin)
    ok = int(dis_df["LastDaily"].notna().sum())
    st.success(f"ØªØ­ÙˆÙŠÙ„ '{last_col}': {ok}/{len(dis_df)} Ù‚ÙŠÙ…Ø§Ù‹ ØµØ§Ù„Ø­Ø©.")
else:
    dis_df["LastDaily"] = pd.NaT
    st.warning("âš ï¸ Ù„Ù… ÙŠÙØ¹Ø«Ø± Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ 'Ø¢Ø®Ø± Ø§ØªØµØ§Ù„' â€” Ø³ÙŠÙØªØ±Ùƒ ÙØ§Ø±ØºÙ‹Ø§.")

# ================= Read Tasks =================
def load_task_files(files, kind_label: str) -> pd.DataFrame:
    if not files:
        return pd.DataFrame()
    frames = []
    for f in files:
        df = pd.read_excel(f)
        premise_col = pick_col(df, ["Premise", "Utility Site Id", "Ø±Ù‚Ù… Ø§Ù„Ù…ÙƒØ§Ù†"])
        reg_col     = pick_col(df, ["Task Registration Date Time", "Request Registration Date Time",
                                    "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©", "ØªØ§Ø±ÙŠØ® ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨"])
        close_col   = pick_col(df, ["Task Closed Time", "Task Completed Time",
                                    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù‚ÙØ§Ù„", "ÙˆÙ‚Øª Ø¥Ù‚ÙØ§Ù„ Ø§Ù„Ù…Ù‡Ù…Ø©"])
        status_col  = pick_col(df, ["Task Status", "Request Status", "Ø§Ù„Ø­Ø§Ù„Ø©",
                                    "Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‡Ù…Ø©", "Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨"])
        result_col  = pick_col(df, ["Technician's Result", "Final Result",
                                    "Final Result (Dispatcher's Result)", "Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©", "Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙÙ†ÙŠ"])

        if not premise_col:
            st.warning(f"ØªØ¬Ø§Ù‡Ù„ '{getattr(f,'name','file')}' â€” Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Premise/Utility Site Id.")
            continue

        tmp = pd.DataFrame()
        tmp["_KEY_PREMISE"] = df[premise_col].astype(str).str.strip()
        tmp["reg_time"]   = smart_parse_datetime(df[reg_col],   excel_origin=excel_origin) if (reg_col   and reg_col   in df.columns) else pd.NaT
        tmp["close_time"] = smart_parse_datetime(df[close_col], excel_origin=excel_origin) if (close_col and close_col in df.columns) else pd.NaT
        tmp["status"]     = df[status_col].astype(str) if (status_col and status_col in df.columns) else np.nan
        tmp["result"]     = df[result_col].astype(str) if (result_col and result_col in df.columns) else np.nan
        tmp["bucket"]     = kind_label
        tmp["source"]     = getattr(f, "name", kind_label)
        frames.append(tmp)
    if not frames:
        return pd.DataFrame()
    return pd.concat(frames, ignore_index=True, sort=False)

insp_df  = load_task_files(insp_files,  "ÙØ­Øµ")
mnt_df   = load_task_files(mnt_files,   "ØµÙŠØ§Ù†Ø©")

# ================= Summaries (latest/first) =================
def latest_by_key(df: pd.DataFrame, date_col: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["_KEY_PREMISE"])
    d = df.copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    # Ø£Ø­Ø¯Ø« Ø³Ø¬Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø«Ù… reg_time ÙƒØªØ¹Ø²ÙŠØ²
    d["_sort"] = d[date_col].fillna(d.get("reg_time"))
    d = d.sort_values(["_KEY_PREMISE", "_sort", "reg_time"], na_position="last")
    idx = d.groupby("_KEY_PREMISE").tail(1).index
    return d.loc[idx].drop(columns=["_sort"], errors="ignore")

def first_by_key(df: pd.DataFrame, date_col: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["_KEY_PREMISE"])
    d = df.copy()
    d[date_col] = pd.to_datetime(d[date_col], errors="coerce")
    d = d.sort_values(["_KEY_PREMISE", date_col])
    idx = d.groupby("_KEY_PREMISE").head(1).index
    return d.loc[idx]

# Ø£Ø­Ø¯Ø« ÙØ­Øµ/Ø£ÙˆÙ„ ØµÙŠØ§Ù†Ø©/Ø¢Ø®Ø± ØµÙŠØ§Ù†Ø©
insp_latest = latest_by_key(insp_df, "close_time" if "close_time" in insp_df.columns else "reg_time")
mnt_first   = first_by_key(mnt_df, "reg_time") if not mnt_df.empty else pd.DataFrame(columns=["_KEY_PREMISE"])
mnt_latest  = latest_by_key(mnt_df, "close_time" if "close_time" in mnt_df.columns else "reg_time")

# Ø£Ø¹Ù„Ø§Ù… Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ù„Ù„ØµÙŠØ§Ù†Ø©
def is_closed_status(s: pd.Series, closed_terms: set[str]) -> pd.Series:
    return s.astype(str).str.strip().str.lower().isin(closed_terms)

mnt_latest = mnt_latest.copy()
if not mnt_latest.empty:
    mnt_latest["_closed"] = mnt_latest["close_time"].notna() | is_closed_status(mnt_latest["status"], CLOSED_TERMS)

# ================= Join with disconnected =================
base = dis_df[["_KEY_PREMISE", "LastDaily"]].copy()
base = base.merge(insp_latest[["_KEY_PREMISE","reg_time","close_time","status","result"]]
                  .rename(columns={"reg_time":"insp_reg","close_time":"insp_close","status":"insp_status","result":"insp_result"}),
                  on="_KEY_PREMISE", how="left")
base = base.merge(mnt_first[["_KEY_PREMISE","reg_time"]].rename(columns={"reg_time":"mnt_first_reg"}),
                  on="_KEY_PREMISE", how="left")
base = base.merge(mnt_latest[["_KEY_PREMISE","reg_time","close_time","status","result","_closed"]]
                  .rename(columns={"reg_time":"mnt_last_reg","close_time":"mnt_last_close","status":"mnt_last_status","result":"mnt_last_result","_closed":"mnt_closed"}),
                  on="_KEY_PREMISE", how="left")

# Ù…Ø¤Ù‚ØªØ§Øª
base["days_from_insp_to_mnt"] = (base["mnt_first_reg"] - base["insp_close"]).dt.days
base["insp_done"]   = base["insp_close"].notna()
base["has_mnt"]     = base["mnt_first_reg"].notna()
base["mnt_open"]    = base["has_mnt"] & ~base["mnt_closed"].fillna(False)
base["mnt_closed"]  = base["mnt_closed"].fillna(False)

# ================= Inspection-focused KPIs =================
st.markdown("## ğŸ§° Ù…Ø¤Ø´Ø±Ø§Øª Ø¬Ù‡ÙˆØ¯ Ø§Ù„ÙØ­Øµ")
insp_count = int(base["insp_done"].sum())
insp_rate  = 100.0 * (base["insp_done"].sum() / len(base)) if len(base) else 0.0
insp_dur   = (insp_df["close_time"] - insp_df["reg_time"]).dt.days.dropna()
avg_insp_days = float(insp_dur.mean()) if not insp_dur.empty else 0.0

k1, k2, k3 = st.columns(3)
k1.metric("Ø¹Ø¯Ø§Ø¯Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© ØªÙ… ÙØ­ØµÙ‡Ø§", f"{insp_count:,}")
k2.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ÙØ­Øµ Ù…Ù† ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„Ø©", f"{insp_rate:.1f}%")
k3.metric("Ù…ØªÙˆØ³Ø· Ù…Ø¯Ø© Ø§Ù„ÙØ­Øµ (Ø£ÙŠØ§Ù…)", f"{avg_insp_days:.1f}")

# ================= Reports =================
st.markdown("## ğŸ“‹ ØªÙ‚Ø§Ø±ÙŠØ± ØªØ´ØºÙŠÙ„ÙŠØ© ØªÙØ¨Ø±Ø² Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø§Ù„ØµÙŠØ§Ù†Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ")

# 1) ÙÙØ­ØµØª ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ ØµÙŠØ§Ù†Ø©
r1 = base[(base["insp_done"]) & (~base["has_mnt"])].copy()
r1 = r1.sort_values(["insp_close"], ascending=[False])

# 2) ØµÙŠØ§Ù†Ø© Ù…ÙÙ‚ÙÙ„Ø© ÙˆÙ…Ø§ Ø²Ø§Ù„ ØºÙŠØ± Ù…ØªØµÙ„ (Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø§ Ø²Ø§Ù„ Ø¶Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„ÙŠÙ† = Ø¯Ù„ÙŠÙ„ Ø¹Ø¯Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©)
r2 = base[(base["insp_done"]) & (base["mnt_closed"])].copy()
# (ÙˆØ¬ÙˆØ¯Ù‡ Ù‡Ù†Ø§ ÙÙŠ Ù…Ù„Ù ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„ÙŠÙ† ÙŠÙƒÙÙŠ Ù„Ù„Ø¯Ù„Ø§Ù„Ø© Ø£Ù†Ù‡ Ù…Ø§ Ø¹Ø§Ø¯ ÙŠØªØµÙ„)

# 3) ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ (Ù„Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„ØµÙŠØ§Ù†Ø©)
r3 = base[(base["insp_done"]) & (base["mnt_open"])].copy()

# Ø£Ø¹Ù…Ø¯Ø© Ø¹Ø±Ø¶ Ù…ÙØ¶Ù„Ø©
cols = ["_KEY_PREMISE", "LastDaily",
        "insp_reg","insp_close","insp_status","insp_result",
        "mnt_first_reg","mnt_last_reg","mnt_last_close","mnt_last_status","mnt_last_result",
        "days_from_insp_to_mnt"]

def show_table_download(title, df, fname):
    st.markdown(f"### {title}  \n({len(df):,}) Ø³Ø¬Ù„")
    st.dataframe(df, use_container_width=True)
    st.download_button(
        f"â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ ({fname}.xlsx)",
        data=to_excel_download(df),
        file_name=f"{fname}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
    st.markdown("---")

show_table_download("1) Ù…ÙØ­ÙˆØµØ© ÙˆÙ„Ø§ ÙŠÙˆØ¬Ø¯ Ù„Ù‡Ø§ ØµÙŠØ§Ù†Ø©", r1[cols], "inspected_no_maintenance")
show_table_download("2) ØµÙŠØ§Ù†Ø© Ù…ÙÙ‚ÙÙ„Ø© ÙˆÙ…Ø§ Ø²Ø§Ù„ ØºÙŠØ± Ù…ØªØµÙ„", r2[cols], "maintenance_closed_still_disconnected")
show_table_download("3) ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ", r3[cols], "maintenance_open_post_inspection")

# ================= Optional quick counts =================
st.markdown("## ğŸ” Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹")
c1, c2, c3 = st.columns(3)
c1.metric("Ù…ÙØ­ÙˆØµØ© Ø¨Ø¯ÙˆÙ† ØµÙŠØ§Ù†Ø©", len(r1))
c2.metric("ØµÙŠØ§Ù†Ø© Ù…ÙÙ‚ÙÙ„Ø© â€” Ù…Ø§ Ø²Ø§Ù„ ØºÙŠØ± Ù…ØªØµÙ„", len(r2))
c3.metric("ØµÙŠØ§Ù†Ø© Ù…ÙØªÙˆØ­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ÙØ­Øµ", len(r3))

st.caption("ÙˆØ¬ÙˆØ¯ Premise ÙÙŠ Ù…Ù„Ù ØºÙŠØ± Ø§Ù„Ù…ØªØµÙ„ÙŠÙ† ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ Ù…Ø§ Ø²Ø§Ù„ ØºÙŠØ± Ù…ØªØµÙ„ Ø­ØªÙ‰ Ù„Ø­Ø¸Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±.")
